{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Thesis Project: collecting tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This is a 'jupyter notebook': a certain kind of program you can use to develop your own software applications. In this notebook we will use the Python computer language and the Twitter API ('application programmer interface') to automatically collect and analyse tweets. \n",
    "\n",
    "This notebook contains cells, i.e., snippets of either code or normal text. We use the text cells (like the current one) to explain what is going on. You can edit a cell by clicking on it. After you made the changes, you can either click 'run' above, or press shift+enter, to execute what is written. In the case of a text cell, this will just display the text in the correct format (try it with this cell!); in case of a code cell, the code will be executed. \n",
    "\n",
    "The next cell will be a code cell where we ask the computer to print a simple sentence for us. Try to change this sentence and then execute the code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note: to use this program, you have to execute all the code in all the cells in the correct order. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about using jupyter notebooks, look for a tutorial online (e.g., https://www.dataquest.io/blog/jupyter-notebook-tutorial/). Most of the questions you have or the problems you encounter will also be solved through a simple google search with the correct keywords.\n",
    "\n",
    "**However, if you have other questions or any problems that you really don't know how to solve, please contact us on Slack and we'll be happy to help or to schedule a meeting. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import successful\n"
     ]
    }
   ],
   "source": [
    "from twython import Twython\n",
    "#if this results in an error, you need to install twython first. See guideline document.\n",
    "print('import successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to connect to Twitter using the correct passwords/keys. There is a limit on how many tweets you can collect each 15 minutes (this makes sure the Twitter servers are not overloaded, amongst other reasons).  Running the code below 'logs you in' to the Twitter application. If all goes well, the output should show information on the number of calls ('questions we can ask') we can still perform these 15 minutes. With each call, you can collect 100 Tweets. \n",
    "e.g.: {'/search/tweets': {'limit': 450, 'remaining': 443, 'reset': 1568288620}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/search/tweets': {'limit': 450, 'remaining': 450, 'reset': 1576581488}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APP_KEY = 'yN3VbAb8QZdzD5GPkVuOHLfMN'         #API key\n",
    "APP_SECRET = 'YRdyk39bx9iRPQBhK2Nh1fT32JdGYTrEhqxcEbcpLMIxbT7wKh'   #API secret key\n",
    "twitter = Twython(APP_KEY, APP_SECRET, oauth_version=2)\n",
    "ACCESS_TOKEN = twitter.obtain_access_token()\n",
    "\n",
    "twitter = Twython(APP_KEY, access_token=ACCESS_TOKEN)\n",
    "twitter.get_application_rate_limit_status()['resources']['search']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the code, we will start 'streaming' tweets: collecting newly created tweets based on certain criteria. These tweets will then be saved in a csv file, a file format that you can open with excel, pages, etc. \n",
    "\n",
    "Every time you want to start streaming, run the code in the cells below. It migth take a while before a first tweet is discovered, so there's nothing wrong if no tweet shows up for a while. If a lot of tweets are streamed (like, e.g., when you would use a keyword like 'Trump' or 'Brexit'); make sure to halt the program in time.\n",
    "\n",
    "New tweets will automatically be added to a file with the filename as specified below. You can change the filename (but do keep the extension '.csv'). This file will be created once a first tweet that matches the criteria is discovered, and tweets will be added to the same file regardless of whether you restarted the application in between. The file will be generated in the same folder as the folder where these notebooks are located. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import TwythonStreamer\n",
    "import csv\n",
    "import os.path\n",
    "\n",
    "filename = 'collected_tweets.csv' #change the filename here \n",
    "delimiter = ';' #change this to ';' or ',' if your software (like excel, numbers...) doesn't show your data in columns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, we first specify what will happen if we find a tweet that matches our criteria. Currently, it will tell us when a new tweet is collected. If it's not a retweet, its date, place and text will be written to file.\n",
    "\n",
    "There's a lot more information you can access for each tweet. If you want to save more than the date, place and text (e.g., the name of the user) please go to https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object and consult the section 'Tweet Data Dictionary'. List all the properties you want to save to file, and contact us so we can update this part of the code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStreamer(TwythonStreamer):\n",
    "    def on_success(self, data):\n",
    "        print(\"-------new tweet collected!\")\n",
    "        \n",
    "        if 'retweeted_status' in data:\n",
    "            print(\"but it's a retweet, so we ignore it...\")\n",
    "        else:\n",
    "            \n",
    "            #first we print selected information about the tweet so you can follow what's happing\n",
    "            if data.get('extended_tweet')!= None and data['extended_tweet'].get('full_text')!= None :\n",
    "                print([data['created_at'],data['place'], data['extended_tweet']['full_text']])\n",
    "            else:\n",
    "                print([data['created_at'],data['place'], data['text']])\n",
    "             \n",
    "            \n",
    "            #below we process the information and write it to file\n",
    "            file_exists = os.path.isfile(filename);\n",
    "            with open(filename,'a', encoding='utf-8') as f: #this will add the newly collected tweets to your dataset ('a' = append) \n",
    "                writer = csv.writer(f, delimiter=';')\n",
    "                \n",
    "                ##write a header to the file if it doesn't exist already\n",
    "                if not file_exists: #if it's a new file, we should create a header \n",
    "                    writer.writerow(['Date','Place_Name','Place_Bounding_Box','Text', 'Tweet_Id', 'IsReplyTo_ID','IsReplyTo_Text', 'Hashtags','Urls','Media','User_Screen_Name', 'User_Id', 'User_Followers_Count', 'Checked_Status_At','Retweets_Count','Favourites_Count']) #this is the document header\n",
    "                \n",
    "    \n",
    "                ##format all the results in a new row to append to the file\n",
    "                \n",
    "                #add a value for the date and time of creation of the tweet\n",
    "                row_to_write = [data['created_at']]\n",
    "                \n",
    "                #process location information, if present\n",
    "                if data.get('place')== None:\n",
    "                    row_to_write.append('')\n",
    "                    row_to_write.append('')\n",
    "                else:\n",
    "                    row_to_write.append(data['place']['full_name'])\n",
    "                    row_to_write.append(data['place']['bounding_box'])\n",
    "                \n",
    "                #the text is stored depending on the type of tweet. We use the full text if available ('extended_tweet'), otherwise we use the standard text\n",
    "                if data.get('extended_tweet')!= None and data['extended_tweet'].get('full_text')!= None :\n",
    "                    row_to_write.append(data['extended_tweet']['full_text'])\n",
    "                else:\n",
    "                    row_to_write.append(data['text'])\n",
    "                      \n",
    "                #then we can add a column for the tweet ID\n",
    "                row_to_write.append(data['id'])   \n",
    "                \n",
    "                #if the tweet is a reply, store the original tweet ID\n",
    "                row_to_write.append(data['in_reply_to_status_id'])  \n",
    "            \n",
    "                if(data['in_reply_to_status_id']):  #if the tweet is a reply, the original text can be fetched later\n",
    "                    row_to_write.append('original text to be fetched')\n",
    "                else: #if the tweet is a not reply, this column can be left empty\n",
    "                    row_to_write.append('')\n",
    "                                \n",
    "                #then we can process the hashtags:\n",
    "                hashtags_as_strings = ''\n",
    "                for x in data['entities']['hashtags']:\n",
    "                    hashtags_as_strings = hashtags_as_strings + ', ' + x['text']\n",
    "                row_to_write.append(hashtags_as_strings)    \n",
    "                                \n",
    "                #then we can process the urls:\n",
    "                urls_as_strings = ''\n",
    "                for x in data['entities']['urls']:\n",
    "                    urls_as_strings = urls_as_strings + ', ' + x['url']\n",
    "                row_to_write.append(urls_as_strings)\n",
    "                                \n",
    "                #presence of media \n",
    "                if data['entities'].get(\"media\") != None:\n",
    "                      row_to_write.append('yes')\n",
    "                else:\n",
    "                    row_to_write.append('no')\n",
    "                \n",
    "                \n",
    "                #user information      \n",
    "                row_to_write.append(data['user']['screen_name'])\n",
    "                row_to_write.append(data['user']['id']) \n",
    "                row_to_write.append(data['user']['followers_count'])\n",
    "                                \n",
    "                \n",
    "                #empty values that will later be replaced with retweet count etc. \n",
    "                row_to_write.append('')\n",
    "                row_to_write.append('')\n",
    "                row_to_write.append('')\n",
    "         \n",
    "            \n",
    "                #finally, write everything to file             \n",
    "                writer.writerow(row_to_write)\n",
    "                                \n",
    "    def on_error(self, status_code, data):\n",
    "        #print(data)\n",
    "        print(status_code)\n",
    "        # self.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we connect to the twitter stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "OAUTH_TOKEN = '1100028871259377670-qtcMTW2ereJ3A0KIvFguWu0ZmW0n8k'\n",
    "OAUTH_TOKEN_SECRET = 'wnPYmWOds9xD1i1CM9K8gfzMNZ26QoBmXW4JSSA81faRF'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you execute the next cell, the streaming will start. This is also the place where you can edit the criteria you want to 'filter' the stream on. There's different types of filters you can use (at the same time):\n",
    "\n",
    "\n",
    "\n",
    "**follow** \t(optional): \tA comma separated list of user IDs, indicating the users to return statuses for in the stream. \n",
    "\n",
    "**track** (optional): \tKeywords to track. Phrases of keywords are specified by a comma-separated list. \n",
    "\n",
    "**locations** \t(optional): \tSpecifies a set of bounding boxes to track. \n",
    "\n",
    "see https://developer.twitter.com/en/docs/tweets/filter-realtime/api-reference/post-statuses-filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = MyStreamer(APP_KEY, APP_SECRET,\n",
    "                    OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "stream.statuses.filter(track='job', tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check status: look up the favorites and retweets counts after given time period (and, if a reply, add the text of the original tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to determine, for tweets we collected in the past, the number of times they have been retweeted and the number of times they have been favorited. To have a fair comparison between tweets this should always be done in more or less the same 'time window'. I.e., here we choose to use always 10 to 14 days later. This means that once you collected tweets you should run this code at least once between 10 and 14 days later. There are 3 possible results: \n",
    "- the tweet is created between 10 to 14 days ago: great! We look up its counts. \n",
    "- the tweet is created less than 10 days ago: the time window in which you should run this code again is added in the file.\n",
    "- the tweet is created less than 14 days ago and hasn't been checked: you missed the window and the tweet has expired. These tweets should be excluded from your analysis (please contact us so we can help). \n",
    "\n",
    "In addition to this, this code will add the original tweet's text to the dataset if the tweet was a reply (independent of the number of days).\n",
    "\n",
    "Note that the updated dataset will be stored in a new file. You can choose the filename below. **Do not give it the same name as your original file!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we will first determine the correct time window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will look up the tweets created between: \n",
      "2019-11-14 12:14:00.425579\n",
      "and \n",
      "2019-11-18 12:14:00.425579\n"
     ]
    }
   ],
   "source": [
    "#the file of collected tweets that you want to get the counts for:\n",
    "filename = 'collected_tweets.csv' #change the filename here \n",
    "\n",
    "#the file with the updated datatset\n",
    "temp_copy_file = 'collected_tweets_updated_thuNov28.csv' #change the filename here (different from original!)\n",
    "\n",
    "\n",
    "#the time period we consider (from 'max_days_back' days ago to 'min_days_back' days ago  )\n",
    "max_days_back = 14 #days ## do not change this\n",
    "min_days_back = 10 #days ## do not change this\n",
    "\n",
    "current_date = datetime.now()\n",
    "max_date_back = current_date - timedelta(days=max_days_back)\n",
    "min_date_back = current_date - timedelta(days=min_days_back)\n",
    "\n",
    "print('we will look up the tweets created between: ')\n",
    "print(max_date_back)\n",
    "print('and ')\n",
    "print(min_date_back)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_Date = 0\n",
    "col_ID = 4\n",
    "col_isReply_ID = 5\n",
    "col_isReply_Text = 6 \n",
    "col_Checked_Status_At = 13\n",
    "col_Retweets_Count= 14\n",
    "col_Favourites_Count = 15\n",
    "\n",
    "with open(filename, mode='r', encoding='utf-8') as csv_file:\n",
    "\n",
    "    csv_reader = csv.reader(csv_file, delimiter=delimiter)\n",
    "    line_count = 0\n",
    "    \n",
    "    with open (temp_copy_file,'w', encoding='utf-8') as temp_csv_copy:\n",
    "        \n",
    "        wtr = csv.writer(temp_csv_copy)\n",
    "\n",
    "        for tweet in csv_reader:\n",
    "            if line_count > 0: #skip the header\n",
    "\n",
    "                #get the time the tweet was created\n",
    "                time_of_creation = datetime.strptime(tweet[col_Date],  \"%a %b %d %H:%M:%S %z %Y\") #'%a %b %d %H:%M:%S %Y')\n",
    "                time_of_creation = time_of_creation.replace(tzinfo=None)\n",
    "                \n",
    "                #did we already get the counts?\n",
    "                if not tweet[col_Retweets_Count]: #we didn't check before\n",
    "                    #is this time is within our bounds, fetch the original tweet from twitter and check its counts\n",
    "                    if time_of_creation > max_date_back and time_of_creation < min_date_back:\n",
    "                        ID = tweet[col_ID]\n",
    "                        fetched_tweet = twitter.show_status(id=ID)\n",
    "                        tweet[col_Retweets_Count] = fetched_tweet['retweet_count']\n",
    "                        tweet[col_Favourites_Count] = fetched_tweet['favorite_count']\n",
    "                        tweet[col_Checked_Status_At] =  current_date\n",
    "                    else:\n",
    "                        if time_of_creation < max_date_back: #the tweet wasn't checked within the bounds and has now expired\n",
    "                             tweet[col_Checked_Status_At] = 'expired'; \n",
    "                        else:\n",
    "                            min_date = time_of_creation + timedelta(days=min_days_back)\n",
    "                            min_date = min_date.strftime(\"%d/%m/%Y, %H:%M:%S\")\n",
    "                            max_date = time_of_creation + timedelta(days=max_days_back)\n",
    "                            max_date = max_date.strftime(\"%d/%m/%Y,, %H:%M:%S\") \n",
    "                            tweet[col_Checked_Status_At] = 'to be checked between ' + min_date + ' and ' + max_date\n",
    "                            \n",
    "                #is the tweet a reply, then add the original tweet's text\n",
    "                if tweet[col_isReply_ID]:\n",
    "                    fetched_tweet = twitter.show_status(id=tweet[col_isReply_ID])\n",
    "                    tweet[col_isReply_Text] = fetched_tweet['text']\n",
    "\n",
    "            line_count += 1\n",
    "            wtr.writerow(tweet);\n",
    "    \n",
    "csv_file.close()\n",
    "temp_csv_copy.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
